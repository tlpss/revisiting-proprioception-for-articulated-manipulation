<!DOCTYPE html>
<html>

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2101EYYX5Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-2101EYYX5Q');
  </script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Revisiting Proprioceptive Sensing for Articulated Object Manipulation">
  <meta property="og:title" content="Revisiting Proprioceptive Sensing for Articulated Object Manipulation" />
  <meta property="og:description" content="Revisiting Proprioceptive Sensing for Articulated Object Manipulation" />
  <meta property="og:url" content="https://tlpss.github.io/revisiting-proprioception-for-articulated-manipulation/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Revisiting Proprioceptive Sensing for Articulated Object Manipulation">
  <meta name="twitter:description" content="Revisiting Proprioceptive Sensing for Articulated Object Manipulation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords"
    content="robotic manipulation, articulated object manipulation, proprioceptive sensing, robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Revisiting Proprioceptive Sensing for Articulated Object Manipulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Revisiting Proprioceptive Sensing for Articulated Object
              Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tlpss.github.io/" target="_blank">Thomas Lips</a>,</span>
              <span class="author-block">
                <a href="https://airo.ugent.be/members/francis/" target="_blank">Francis wyffels</a></span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">AI and Robotics Lab - Ghent University<br> <a
                  href="https://sites.google.com/view/icra2023embracingcontacts/home?authuser=0">Embracing Contacts
                  workshop</a> - ICRA 2023</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2305.09584.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/tlpss/cabinet-manipulation-eef" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2305.09584" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>


                <!-- posteer PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/ICRA23-poster.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video style="display: block; margin: auto;" poster="" id="tree" autoplay controls muted loop width="100%">
          <!-- Your video here -->
          <source src="static/videos/ex3-4x.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          We create a system that uses proprioceptive sensing to open articulated objects, such as this
          IKEA cabinet,
          with a position-controlled robot and a parallel gripper.
          By showing this is a viable approach, we hope to inspire future work to revisit proprioceptive
          sensing for
          articulated object manipulation.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Assistive robots will need to interact with articulated objects such as cabinets or
              microwaves. Early work
              on creating systems for doing so used proprioceptive sensing to estimate joint mechanisms
              during contact.
              However, nowadays, almost all systems use only vision and no longer consider proprioceptive
              information
              during contact. We believe that proprioceptive information during contact is a valuable
              source of
              information and did not find clear motivation for not using it in the literature. Therefore,
              in this
              paper, we create a system that, starting from a given grasp, uses proprioceptive sensing to
              open cabinets
              with a position-controlled robot and a parallel gripper. We perform a qualitative evaluation
              of this
              system, where we find that slip between the gripper and handle limits the performance.
              Nonetheless, we
              find that the system already performs quite well. This poses the question: should we make
              more use of
              proprioceptive information during contact in articulated object manipulation systems, or is
              it not worth
              the added complexity, and can we manage with vision alone? We do not have an answer to this
              question, but
              we hope to spark some discussion on the matter.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- overview figure -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-3">System Overview</h2>

            <img src="static/images/proprio-cabinets-overview.png" alt="Overview" class="center-image">
            <div class="level-set has-text-justified">
              <p>
                Under the assumption of a rigid transform between the gripper and the handle, we can use the gripper
                poses to estimate the joint twist. We modify a Factor Graph proposed by <a href="https://arxiv.org/abs/2205.03721">Heppert et Al.</a> to do so. An
                admittance controller is then used to deal with estimation errors and open the articulated object. Subsequent gripper poses allow to refine the joint estimation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Results -->
  <section class="section hero is-small is-light">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title-is-3"> Results </h2>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">

          <video style="display: block; margin: auto;" poster="" id="tree" autoplay controls muted width="100%">
            <!-- Your video here -->
            <source src="static/videos/ex5-4x.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video style="display: block; margin: auto;" poster="" id="tree" autoplay controls muted width="100%">
            <!-- Your video here -->
            <source src="static/videos/ex2-4x.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column">
          <video style="display: block; margin: auto;" poster="" id="tree" autoplay controls muted width="100%">
            <!-- Your video here -->
            <source src="static/videos/ex4-4x.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column">
          <video style="display: block; margin: auto;" poster="" id="tree" autoplay controls muted width="100%">
            <!-- Your video here -->
            <source src="static/videos/ex1-4x.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <p>We find that our system is able to open various articulated objects, irrespective of their relative pose
              or surroundings.
              This reconfirms the potential of proprioceptive sensing for articulated object manipulation.

              Nonetheless, we observe a few limtations, the first of which is directly related to the use of
              proprioceptive sensing:
              <br>
              <br>
              Slip between the gripper and handle can occur (see for example the video of the upside down cabinet) and
              deteriorates the articulation estimation. This does not necessarily result in a failure to open the
              object, yet it will need to be addressed
              to obtain more robust systems.
              <br>
              <br>
              Using fixed grasps, i.e. grasping the handle once and then attempting to open it without regrasping, is
              not always possible. For some objects (such as the small oven in the videos above), it will result in
              collisions with the environment. It also aggravates workspace limitations of the robot, where the robot
              cannot fully open the object due to its workspace limitations. Both situations can be addressed by
              regrasping the object as needed.

              <br>
              <br>


              Finally, it is important to note that our system takes about 2 minutes to open an articulated object. Even
              though this is comparable to other works, it is 2 orders of magnitudes slower than humans. 

            <p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Open Questions -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title-is-3"> Open Questions </h2>
          </div>
        </div>
      </div>



      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <p> We have seen that proprioceptive sensing can be used to open articulated objects.
              However, it introduces additional complexity and slip needs to be handled. Furthermore, visual information
              is still required to find appropriate grasp poses. This brings us the the main question:
              <b>
                Should we use proprioceptive sensing for articulated object manipulation? Or can we manage with vision
                  alone? And if we do use proprioceptive sensing, how can we efficiently combine it with visual
                  information?
              </b>
              <br>
              We believe these are exciting questions and offer directions for future work.
              <br>
              <br>
              Feel free to contact us if you have an opinion on the matter or if you have any other questions or
              comments!
            <p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <!-- use https://bibtex.online/ to check -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{lips2023proprioceptive-articulated-manipulation,
        author    = {Lips, Thomas and wyffels, Francis},
        title     = {Revisiting Proprioceptive Sensing for Articulated Object Manipulation},
        booktitle = {Embracing Contacts workshop - IEEE International Conference on Robotics and Automation 2023 (ICRA)},
        year = {2023},
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              You are free to borrow the of this website, we just ask that you link back to this page in
              the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>